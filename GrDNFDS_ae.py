# -*- coding: utf-8 -*-
"""FCM_VAE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14Ok5Ra3cC11mkCY5hItpiMnwMPcNf3SE

# I. Set up environment

- Load data via URL
"""


"""- Load data via directory"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
from glob import glob
from PIL import Image
import torch
from torch.autograd import Variable
from torchvision import datasets, transforms
import numpy as np
import torch.nn as nn
import pandas as pd
from torch.utils.data import Dataset
# %matplotlib inline

class FerDataset(Dataset):
    def __init__(self, images, labels, transform=None):
        self.images = images
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()

        img = np.array(self.images[idx])
        img = Image.fromarray(img)

        if self.transform:
            img = self.transform(img)

        label = torch.tensor(self.labels[idx]).type(torch.long)
        sample = (img, label)

        return sample



def load_data(path='fer2013/fer2013.csv'):
    fer2013 = pd.read_csv(path)
    emotion_mapping = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}

    return fer2013, emotion_mapping



def prepare_data(data):
    """ Prepare data for modeling
        input: data frame with labels und pixel data
        output: image and label array """

    image_array = np.zeros(shape=(len(data), 48, 48))
    image_label = np.array(list(map(int, data['emotion'])))

    for i, row in enumerate(data.index):
        image = np.fromstring(data.loc[row, 'pixels'], dtype=int, sep=' ')
        image = np.reshape(image, (48, 48))
        image_array[i] = image

    return image_array, image_label



def get_data(train_transform=None,test_transform=None):
    if train_transform is None and test_transform is None:
        mu, st = 0, 255
        train_transform = transforms.Compose([
                                              transforms.RandomCrop(48, padding=4, padding_mode='reflect'),
                                              transforms.RandomAffine(
                                                  degrees=30,
                                                  translate=(0.01, 0.12),
                                                  shear=(0.01, 0.03),
                                              ),
                                              transforms.RandomHorizontalFlip(),
                                                transforms.TenCrop(48),
            transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),
            transforms.Lambda(lambda tensors: torch.stack([transforms.Normalize(mean=(mu,), std=(st,))(t) for t in tensors])),
            transforms.Lambda(lambda tensors: torch.stack([transforms.RandomErasing(p=0.5)(t) for t in tensors])),
                                              ])

        test_transform = transforms.Compose([transforms.TenCrop(48),
        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),
        transforms.Lambda(lambda tensors: torch.stack([transforms.Normalize(mean=(mu,), std=(st,))(t) for t in tensors])),])
    else:
        train_transform = train_transform
        test_transform = test_transform
    path='fer2013/fer2013.csv'
    fer2013, emotion_mapping = load_data(path)

    xtrain, ytrain = prepare_data(fer2013[fer2013['Usage'] == 'Training'])
    xtest, ytest = prepare_data(fer2013[fer2013['Usage'] == 'PublicTest'])

    train_data =FerDataset(xtrain, ytrain,train_transform)
    test_data = FerDataset(xtest, ytest, test_transform)
    return train_data,test_data

train_data,test_data = get_data()

"""total samples of training data"""

from torch.utils.data import DataLoader
data_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2)
test_loader = DataLoader(test_data, batch_size=32, shuffle=True, num_workers=2)

it = iter(data_loader)
first_batch = next(it)

"""given mean and standard deviation of the distribution, we used random sample to
sample a tensor from the distribution

# III. FCM

this kl-divergence (22) uses membership matrix qij to approximate student's-t Distribution pij in DEC,IDEC structures


$$
\Large \min KL(P||Q) = \min \sum_{i=1}^N \sum_{j=1}^c p_{ij}\log\frac{p_{ij}}{q_{ij}} \qquad (22)
$$
"""

loss_kl = nn.KLDivLoss(reduction='sum')
def KL_divergence(p,q):
  return loss_kl(q.log(), p) / q.shape[0]

"""The desired target pij of the Studentâ€™s-t based membership qij
is defined with (12), which increases the variance of qij and
normalizes qij with respect of the jth center .

$$
\Large p_{ij} = \frac{q_{ij}^2 / \sum_{i}q_{ij}}
{\sum_{k=1}^{c}(q_{ik}^2 / \sum_{i}q_{ik})},
\quad \sum_{j=1}^{c}p_{ij} = 1, \, \forall{i} \qquad (12)
$$
"""

def calculate_p(q):
  w = (q**2/torch.sum(q,0))
  return (w.t()/torch.sum(w,1).t()).t()

"""Minimzing Fuzzy cluster compactness equivalent to minimize within class scatter matrix and maximize between class matrix.From this objective, fuzzy membership qij is given by (21)

$$
\Large q_{ij} = \frac{(\|z_{i} - \mu_{j}\|^2 - \frac{\beta}{\sum_{k=1}^{c}\|\mu_{k} - \overline{\mu}\|^2}\|\mu_{j} - \overline{\mu}\|^2)^{-1/(m-1)}}
{\sum_{k=1}^{c}(\|z_{i} - \mu_{k}\|^2 - \frac{\beta}{\sum_{k=1}^{c}\|\mu_{k} - \overline{\mu}\|^2}\|\mu_{k} - \overline{\mu}\|^2)^{-1/(m-1)}} \qquad (21)
$$
"""

class ClusterAssigment(nn.Module):
  def __init__(self,cluster_number,cluster_center,embedding_dimension):
    super(ClusterAssigment,self).__init__()
    self.cluster_number = cluster_number
    self.embedding_dimension = embedding_dimension
    if cluster_center is None:
      initial_cluster_center = torch.zeros(cluster_number,embedding_dimension,dtype=torch.float).to('cuda')
      nn.init.xavier_uniform_(initial_cluster_center)
    else:
      initial_cluster_center = cluster_center
    self.cluster_center = nn.Parameter(initial_cluster_center,requires_grad=True).to('cuda')
  def forward(self,z):
    batch_size,embedding_dimension = z.size()
    beta = 0.006
    m = 1.5
    power = -1/(m-1)
    norm_squared = torch.sum((z.unsqueeze(1)-self.cluster_center)**2,2)
    norm_squared_mu = torch.sum((self.cluster_center - self.cluster_center.mean())**2,1)
    numerator = (norm_squared-beta/torch.sum(norm_squared_mu)*norm_squared_mu)**power
    q = numerator/torch.sum(numerator,dim=1,keepdim=True)
    return q

"""Define label li,lv are pseudo labels of the maximum value's index of the datapoint with coresponding the C cluster, taking datapoints zi,zv we calculate the affinity between the datapoints when it has the same pseudo-labels or 0 otherwise

$$
\Large s_{iv} =
\begin{cases}
  \exp(-\|z_{i} - z_{v}\|^2 / \sigma) / t, \quad l_i = l_v
  \\\\
  0, \quad l_i \ne l_v
\end{cases}
\qquad (28)
$$
"""

def repeat(tensor, dims):
    if len(dims) != len(tensor.shape):
        raise ValueError("The length of the second argument must equal the number of dimensions of the first.")
    for index, dim in enumerate(dims):
        repetition_vector = [1]*(len(dims)+1)
        repetition_vector[index+1] = dim
        new_tensor_shape = list(tensor.shape)
        new_tensor_shape[index] *= dim
        tensor = tensor.unsqueeze(index+1).repeat(repetition_vector).reshape(new_tensor_shape)
    return tensor

def calculate_affinity(q,z):
  batch,_ = q.size()
  s_iv = torch.zeros(batch,batch).to(z.device)
  labels = torch.argmax(q,axis=1)
  sigma = 1
  t = 0.08
  x = (labels == labels.unsqueeze(1)).nonzero(as_tuple=True)
  s_iv[x] = torch.exp(-torch.linalg.norm(z[x[0]] - z[x[1]], axis=1)**2/sigma)/t
  return s_iv,labels

"""Graph based regularization on hidden features with the assumption that the input data should be consistent with the ones in new representation (for example Z-representation). If the affinity of xi and xv is high then the distance between zi and zv must be low, on the other hand, if affinity of xi and xv is low like zero then the distance between zi and zv can be large .

$$
\Large \min G_z = \min \sum_{i,v=1}^{N} \|z_i - z_v\|^{2} s_{iv}
$$
"""

def graph_based_reg(q,z,s_iv,calculate_siv):
  batch,_ = q.size()
  if calculate_siv:
      s_iv,pseudo_label =  calculate_affinity(q,z)
  else:
      pseudo_label=torch.argmax(q,axis=1)
  x = (pseudo_label == pseudo_label.unsqueeze(1)).nonzero(as_tuple=True)
  l_graph_based_reg = torch.sum(torch.linalg.norm(z.unsqueeze(1)-z.unsqueeze(0),axis=-1)**2*s_iv)
  return l_graph_based_reg, s_iv,pseudo_label

"""# IV. Define and Train Model

The architecture of autoecnoder
"""

class AutoEncoder(nn.Module):

    def __init__(self, z_dims=256, in_channels=1, hid_channels=32):
        super(AutoEncoder, self).__init__()
        self.z_dims = z_dims
        self.encoder = nn.Sequential(
            self.conv_block(in_channels, hid_channels),
            self.conv_block(hid_channels, hid_channels*2),
            self.conv_block(hid_channels*2, hid_channels*4),
            self.conv_block(hid_channels*4, hid_channels*8),
            nn.Conv2d(hid_channels*8, hid_channels*2, 1, 1)
        )

        self.fc1 = nn.Sequential(
            nn.Linear(3*3*hid_channels*2, z_dims)
            # nn.ReLU(),
            # nn.Linear(1024, z_dims)
        )
        self.assigment = ClusterAssigment(7,None,z_dims)

        self.fc2 = nn.Sequential(
            # nn.Linear(z_dims, 1024),
            # nn.ReLU(),
            nn.Linear(z_dims, 3*3*hid_channels*2)
        )

        self.decoder = nn.Sequential(
            nn.Conv2d(hid_channels*2, hid_channels*8, 1, 1),
            self.deconv_block(hid_channels*8, hid_channels*4),
            self.deconv_block(hid_channels*4, hid_channels*2),
            self.deconv_block(hid_channels*2, hid_channels),
            nn.ConvTranspose2d(hid_channels, in_channels, 2, 2),
            nn.Sigmoid()
        )

    def conv_block(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):
        return nn.Sequential(
            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),
            nn.BatchNorm2d(out_channels),
            nn.ReLU()
            # nn.Conv2d(out_channels, out_channels, kernel_size, stride, padding),
            # nn.BatchNorm2d(out_channels),
            # nn.ReLU()
        )

    def deconv_block(self, in_channels, out_channels, kernel_size=2, stride=2, padding=0):
        return nn.Sequential(
            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding=padding),
            nn.BatchNorm2d(out_channels),
            nn.ReLU()
            # nn.Conv2d(out_channels, out_channels, kernel_size, stride, padding),
            # nn.BatchNorm2d(out_channels),
            # nn.ReLU()
        )

    def forward(self, image):
        batch_size = int(image.size()[0]/10)
        x = self.encoder(image)
        x = x.view(len(x), -1)
        z = self.fc1(x)
        q = self.assigment(z)
        x = self.fc2(z)
        x = x.view(len(x), -1, 3, 3)
        x = self.decoder(x)
        return x,z,q

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)
criterion = nn.BCELoss()
ae = AutoEncoder().to(device)
optimizer = torch.optim.Adam(ae.parameters(), lr=0.1, betas=(0.9, 0.999))



plt.rcParams["figure.figsize"] = (16, 8)

from torchvision.utils import make_grid
from tqdm import tqdm
import time
num_class=7
alpha1 = 0.5
alpha2 = 0.2
batch=64

prediction_n_ground = []
S = []

"""Pre-train autencoder"""

def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):
    image_unflat = image_tensor.detach().cpu()
    image_grid = make_grid(image_unflat[:num_images], nrow=5)
    plt.axis('off')
    plt.imshow(image_grid.permute(1, 2, 0).squeeze())
for epoch in range(10):
     print(f"Epoch {epoch}")
     time.sleep(0.5)
     loss_mean=0
     count=0
     for images,r in tqdm(data_loader,position=0, leave=True):
         optimizer.zero_grad() # Clear out the gradients
         images = images.to(device)
         bs, ncrops, c, h, w = images.shape
         merged_images = images.view(-1, c, h, w)
         outputs,z,q = ae(merged_images)
         outputs = outputs.view(bs, ncrops, c,h,w)
         loss = criterion(outputs, images)
         loss_mean+=loss.item()
         count+=1
         loss.backward()
         optimizer.step()
     print('mean loss:',loss_mean/count)

images.size()

features=[]
from sklearn.cluster import KMeans
kmeans = KMeans(7, n_init=20)
for r,(images,_) in enumerate(tqdm(data_loader,position=0,leave=True)):
  images = images.to(device)
  bs, ncrops, c, h, w = images.shape
  merged_images = images.view(-1, c, h, w)
  out,z,q =ae(merged_images)
  features.append(z.detach().cpu())
predicted = kmeans.fit_predict(torch.cat(features).numpy())
cluster_centers = torch.tensor(kmeans.cluster_centers_,dtype=torch.float,requires_grad=True).to(device)

with torch.no_grad():
   ae.assigment.cluster_center.copy_(cluster_centers)

"""Initilize parameters"""

#step 3 initialize
device ='cuda'
for images,_ in tqdm(data_loader,position=0,leave=True):
      ae.eval()
      s_iv = torch.zeros((batch,batch)).to('cuda')
      with torch.no_grad():
          optimizer.zero_grad() # Clear out the gradients
          images = images.to(device)
          bs, ncrops, c, h, w = images.shape
          merged_images = images.view(-1, c, h, w)
          out,z,q = ae(merged_images)

          _,s_iv,p_label = graph_based_reg(q,z,s_iv,True)
          S.append(s_iv)

from itertools import permutations
def remap_labels(pred_labels,true_labels):
    pred_labels,true_labels = np.array(pred_labels),np.array(true_labels)
    assert pred_labels.ndim == true_labels.ndim == 1
    assert len(pred_labels) == len(true_labels)
    cluster_names = np.unique(pred_labels)
    accuracy = 0

    perms = np.array(list(permutations(np.unique(true_labels))))

    remapped_labels = true_labels
    for perm in perms:
        flipped_labels = np.zeros(len(true_labels))
        for label_index,label in enumerate(cluster_names):
            flipped_labels[pred_labels == label] = perm[label_index]

        testAcc = np.sum(flipped_labels == true_labels) / len(true_labels)
        if testAcc > accuracy:
            accuracy = testAcc
            remapped_labels = flipped_labels
    return accuracy,remapped_labels

"""Train the whole model"""

torch.autograd.set_detect_anomaly(True)
#step 4 train
alpha1 = 0.1
alpha2 = 0.8
ae.train()
for epoch in range(100):
    print(f"Epoch {epoch}")
    time.sleep(0.5)
    loss_mean=0
    for r,(images,_) in enumerate(tqdm(data_loader,position=0,leave=True)):
        optimizer.zero_grad() # Clear out the gradients
        S_r= S[r].to(device).to('cuda')
        images = images.to(device)
        bs, ncrops, c, h, w = images.shape
        merged_images = images.view(-1, c, h, w)
        outputs,z,q = ae(merged_images)
        p = calculate_p(q)
        loss_graph_based_reg,_,_ = graph_based_reg(q,z,S_r,False)
        kl = KL_divergence(p,q)
        outputs = outputs.view(bs, ncrops, c,h,w)
        ae_loss = criterion(outputs, images)
        loss = ae_loss + alpha1*kl + alpha2*loss_graph_based_reg
        loss_mean+=loss.item()
        loss.backward()
        optimizer.step()
    print('mean loss:',loss_mean/r)
    preds = []
    gts = []
    ae.eval()
    for r,(images,ground_truth) in enumerate(tqdm(data_loader,position=0,leave=True)):
        optimizer.zero_grad()
        images = images.to(device)
        bs, ncrops, c, h, w = images.shape
        merged_images = images.view(-1, c, h, w)
        out,z,q = ae(merged_images)
        _,s_iv,p_label = graph_based_reg(q,z,S[r].to(device),True)
        S[r]= s_iv.detach().clone()
        preds.append(p_label.squeeze(0).detach().cpu())
        ground_truth = torch.repeat_interleave(ground_truth, repeats=ncrops, dim=0)
        gts.append(ground_truth.squeeze(0).detach().cpu())
    prediction = torch.cat(preds).numpy()
    g_truth = torch.cat(gts).numpy()
    acc = remap_labels(prediction,g_truth)
    print('accuracy:',acc)
    #test
    test_pred = []
    test_gt = []
    for r,(images,ground_truth) in enumerate(tqdm(test_loader,position=0,leave=True)):
        optimizer.zero_grad()
        images = images.to(device)
        bs, ncrops, c, h, w = images.shape
        merged_images = images.view(-1, c, h, w)
        out,z,q = ae(merged_images)
        prediction_test = torch.argmax(q,axis=1)
        test_pred.append(prediction_test.squeeze(0).detach().cpu())
        ground_truth = torch.repeat_interleave(ground_truth, repeats=ncrops, dim=0)
        test_gt.append(ground_truth.squeeze(0).detach().cpu())
    test_pred = torch.cat(test_pred).numpy()
    test_gt = torch.cat(test_gt).numpy()
    acc = remap_labels(test_pred,test_gt)
    print('accuracy:',acc)

len(g_truth),len(prediction),z.size()

torch.save(ae.state_dict(),'/content/ae.pth')
